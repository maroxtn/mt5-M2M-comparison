{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fairseq-test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYmIDW5fN7yS",
        "outputId": "3f97c0a6-1e1f-454b-c9ef-148366e16412"
      },
      "source": [
        "!git clone https://github.com/pytorch/fairseq -q\n",
        "%cd fairseq\n",
        "!pip uninstall numpy -q -y\n",
        "!pip install wandb -q\n",
        "!pip install --editable ./ -q\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fairseq\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 7.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 37.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 34.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 13.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.2MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 133kB 7.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 56.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 47.5MB/s \n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.21.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UldcvGCPOEbv",
        "outputId": "be46061d-e64e-48fc-86d1-525ce959971e"
      },
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSA3MTlTOOWq",
        "outputId": "e54bd278-834e-4fa3-f3ea-1a32c2eeea66"
      },
      "source": [
        "!wget -qq \"https://dl.fbaipublicfiles.com/m2m_100/spm.128k.model\"\n",
        "!wget -qq \"https://dl.fbaipublicfiles.com/m2m_100/data_dict.128k.txt\"\n",
        "!wget -qq \"https://dl.fbaipublicfiles.com/m2m_100/model_dict.128k.txt\"\n",
        "!wget -qq \"https://dl.fbaipublicfiles.com/m2m_100/language_pairs_small_models.txt\"\n",
        "!wget \"https://dl.fbaipublicfiles.com/m2m_100/418M_last_checkpoint.pt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-25 14:25:14--  https://dl.fbaipublicfiles.com/m2m_100/418M_last_checkpoint.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4839402958 (4.5G) [application/octet-stream]\n",
            "Saving to: ‘418M_last_checkpoint.pt’\n",
            "\n",
            "418M_last_checkpoin 100%[===================>]   4.51G  52.8MB/s    in 95s     \n",
            "\n",
            "2021-06-25 14:26:49 (48.7 MB/s) - ‘418M_last_checkpoint.pt’ saved [4839402958/4839402958]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv378z1dOTo5",
        "outputId": "60015834-df7a-4606-dc34-f1ad2da8d091"
      },
      "source": [
        "! pip install sentencepiece -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 26.1MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 18.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 15.8MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 14.4MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 8.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92kB 9.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 7.5MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 7.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 7.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 7.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 7.5MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 7.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163kB 7.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 7.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 307kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0MB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 7.5MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI4C3cr-OVXV"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False \n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIbMRZbMO3ac"
      },
      "source": [
        "\n",
        "PATH_TO_DATASET = \"./\"  #Where you stored the dataset\n",
        "\n",
        "train = pd.read_csv(os.path.join(PATH_TO_DATASET, \"Train.csv\"))\n",
        "\n",
        "#Remove any possible duplicates\n",
        "train = train.drop_duplicates(subset=[\"Yoruba\", \"English\"])\n",
        "\n",
        "#Lowercase and remove trailing spaces\n",
        "train[\"Yoruba\"] = train.apply(lambda x: (x.Yoruba).strip().lower(), axis=1)\n",
        "train[\"English\"] = train.English.apply(lambda x: x.lower())\n",
        "\n",
        "train = train[[\"Yoruba\", \"English\"]]\n",
        "train.columns = [\"input_text\", \"target_text\"]\n",
        "\n",
        "#Train 95% / Validation 5% Split\n",
        "validation = train.sample(frac=0.05).astype(str)\n",
        "train = train.drop(index=validation.index).astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APIKtxs1O7ix"
      },
      "source": [
        "train_txt = \"\\n\".join(train.input_text.values.tolist())\n",
        "\n",
        "file = open(\"yoruba_txt_train.txt\", \"w\")\n",
        "file.write(train_txt)\n",
        "file.close()\n",
        "\n",
        "\n",
        "train_target_txt = \"\\n\".join(train.target_text.values.tolist())\n",
        "\n",
        "file = open(\"english_txt_train.txt\", \"w\")\n",
        "file.write(train_target_txt)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4krKQk7O86J"
      },
      "source": [
        "validation_txt = \"\\n\".join(validation.input_text.values.tolist())\n",
        "\n",
        "file = open(\"yoruba_txt_validation.txt\", \"w\")\n",
        "file.write(validation_txt)\n",
        "file.close()\n",
        "\n",
        "\n",
        "validation_target_txt = \"\\n\".join(validation.target_text.values.tolist())\n",
        "\n",
        "file = open(\"english_txt_validation.txt\", \"w\")\n",
        "file.write(validation_target_txt)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KCtx-R7O_AK",
        "outputId": "00c7916e-c603-45da-f643-c41e9a462f4f"
      },
      "source": [
        "!python fairseq/scripts/spm_encode.py \\\n",
        "        --model spm.128k.model \\\n",
        "        --output_format=piece \\\n",
        "        --inputs=yoruba_txt_train.txt \\\n",
        "        --outputs=train.yo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "skipped 0 empty lines\n",
            "filtered 0 lines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1EQOHv4PCzi",
        "outputId": "cc4dc82e-b99b-4a7a-c6c2-a83847bcda13"
      },
      "source": [
        "!python fairseq/scripts/spm_encode.py \\\n",
        "        --model spm.128k.model \\\n",
        "        --output_format=piece \\\n",
        "        --inputs=english_txt_train.txt \\\n",
        "        --outputs=train.en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "skipped 0 empty lines\n",
            "filtered 0 lines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI5QJgiKPESy",
        "outputId": "919b6ea2-3fc8-4bf9-80af-09b369fabc3e"
      },
      "source": [
        "!python fairseq/scripts/spm_encode.py \\\n",
        "        --model spm.128k.model \\\n",
        "        --output_format=piece \\\n",
        "        --inputs=yoruba_txt_validation.txt \\\n",
        "        --outputs=val.yo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "skipped 0 empty lines\n",
            "filtered 0 lines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGs39gCPPFfO",
        "outputId": "5a1ad509-5c78-45ea-d3b2-667783273515"
      },
      "source": [
        "!python fairseq/scripts/spm_encode.py \\\n",
        "        --model spm.128k.model \\\n",
        "        --output_format=piece \\\n",
        "        --inputs=english_txt_validation.txt \\\n",
        "        --outputs=val.en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "skipped 0 empty lines\n",
            "filtered 0 lines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH_Lh7WFPGZO",
        "outputId": "4fa17aed-39f8-4633-a501-b17b8b52854c"
      },
      "source": [
        "!fairseq-preprocess \\\n",
        "    --source-lang yo --target-lang en \\\n",
        "    --trainpref train \\\n",
        "    --validpref val \\\n",
        "    --thresholdsrc 0 --thresholdtgt 0 \\\n",
        "    --destdir data_bin \\\n",
        "    --srcdict model_dict.128k.txt --tgtdict model_dict.128k.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-25 14:26:58 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data_bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='yo', srcdict='model_dict.128k.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref=None, tgtdict='model_dict.128k.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='train', use_plasma_view=False, user_dir=None, validpref='val', wandb_project=None, workers=1)\n",
            "2021-06-25 14:26:58 | INFO | fairseq_cli.preprocess | [yo] Dictionary: 128112 types\n",
            "2021-06-25 14:26:58 | INFO | fairseq_cli.preprocess | [yo] train.yo: 200 sents, 10039 tokens, 0.538% replaced by <unk>\n",
            "2021-06-25 14:26:58 | INFO | fairseq_cli.preprocess | [yo] Dictionary: 128112 types\n",
            "2021-06-25 14:26:58 | INFO | fairseq_cli.preprocess | [yo] val.yo: 100 sents, 4576 tokens, 0.437% replaced by <unk>\n",
            "2021-06-25 14:26:58 | INFO | fairseq_cli.preprocess | [en] Dictionary: 128112 types\n",
            "2021-06-25 14:26:58 | INFO | fairseq_cli.preprocess | [en] train.en: 200 sents, 5562 tokens, 0.036% replaced by <unk>\n",
            "2021-06-25 14:26:58 | INFO | fairseq_cli.preprocess | [en] Dictionary: 128112 types\n",
            "2021-06-25 14:26:58 | INFO | fairseq_cli.preprocess | [en] val.en: 100 sents, 2547 tokens, 0.0393% replaced by <unk>\n",
            "2021-06-25 14:26:58 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data_bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeffPpqfPH5N"
      },
      "source": [
        "!mkdir checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4RqukuhPKEd"
      },
      "source": [
        "import threading \n",
        "import os\n",
        "\n",
        "\n",
        "def remove_checkpoints():\n",
        "    threading.Timer(5, remove_checkpoints).start()\n",
        "    files = os.listdir(\"checkpoint\")\n",
        "    #print(\"here\")\n",
        "  \n",
        "    for file in files:\n",
        "        if file != \"checkpoint_best.pt\" and file.split(\".\")[-1] ==\"pt\":\n",
        "            os.remove(\"checkpoint/\"+file)\n",
        "            print(\"Removed \" + file)\n",
        "\n",
        "remove_checkpoints()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRqf1RKLinnJ",
        "outputId": "5ee13730-1913-4c16-9044-9d86de5d2577"
      },
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login(key=\"PUT YOUR API KEY HERE\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNA2ZOe9PLFV",
        "outputId": "0d0a433c-884b-4c0a-a667-28c8032855d7"
      },
      "source": [
        "!fairseq-train data_bin \\\n",
        "  --finetune-from-model  \"418M_last_checkpoint.pt\"\\\n",
        "  --save-dir checkpoint \\\n",
        "  --task translation_multi_simple_epoch \\\n",
        "  --encoder-normalize-before \\\n",
        "  --lang-pairs 'yo-en' \\\n",
        "  --batch-size 10 \\\n",
        "  --decoder-normalize-before \\\n",
        "  --encoder-langtok src \\\n",
        "  --decoder-langtok \\\n",
        "  --criterion cross_entropy \\\n",
        "  --optimizer adafactor \\\n",
        "  --lr-scheduler cosine \\\n",
        "  --lr 3e-05 \\\n",
        "  --max-update 40000 \\\n",
        "  --update-freq 2 \\\n",
        "  --save-interval 1 \\\n",
        "  --save-interval-updates 5000 \\\n",
        "  --keep-interval-updates 10 \\\n",
        "  --no-epoch-checkpoints \\\n",
        "  --log-format simple \\\n",
        "  --log-interval 2 \\\n",
        "  --patience 10 \\\n",
        "  --arch transformer_wmt_en_de_big \\\n",
        "  --encoder-layers 12 --decoder-layers 12 \\\n",
        "  --share-decoder-input-output-embed --share-all-embeddings \\\n",
        "  --ddp-backend no_c10d \\\n",
        "  --max-epoch 10 \\\n",
        "  --wandb-project \"Yoruba M2M\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-25 14:27:04 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 2, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'Yoruba M2M', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 10, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 10, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 40000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoint', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': '418M_last_checkpoint.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_wmt_en_de_big', activation_dropout=0.0, activation_fn='relu', adafactor_eps='(1e-30, 1e-3)', adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_wmt_en_de_big', attention_dropout=0.1, azureml_logging=False, batch_size=10, batch_size_valid=10, best_checkpoint_metric='loss', beta1=None, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, clip_threshold=1.0, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='data_bin', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decay_rate=-0.8, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_langtok=True, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_langtok='src', encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, eos=2, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='418M_last_checkpoint.pt', fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, lang_dict=None, lang_pairs='yo-en', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=2, lr=[3e-05], lr_period_updates=-1, lr_scheduler='cosine', lr_shrink=0.1, max_epoch=10, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=40000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=0.0, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adafactor', optimizer_overrides='{}', pad=1, patience=10, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relative_step=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='checkpoint', save_interval=1, save_interval_updates=5000, scale_parameter=False, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_dict=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, t_mult=1.0, target_dict=None, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[2], upsample_primary=1, use_bmuf=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=None, wandb_project='Yoruba M2M', warmup_init=False, warmup_init_lr=-1, warmup_updates=0, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='translation_multi_simple_epoch', activation_dropout=0.0, activation_fn='relu', adafactor_eps='(1e-30, 1e-3)', adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_wmt_en_de_big', attention_dropout=0.1, azureml_logging=False, batch_size=10, batch_size_valid=10, best_checkpoint_metric='loss', beta1=None, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, clip_threshold=1.0, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='data_bin', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decay_rate=-0.8, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_langtok=True, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_langtok='src', encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, eos=2, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='418M_last_checkpoint.pt', fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, lang_dict=None, lang_pairs='yo-en', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=2, lr=[3e-05], lr_period_updates=-1, lr_scheduler='cosine', lr_shrink=0.1, max_epoch=10, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=40000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=0.0, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adafactor', optimizer_overrides='{}', pad=1, patience=10, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relative_step=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='checkpoint', save_interval=1, save_interval_updates=5000, scale_parameter=False, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_dict=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, t_mult=1.0, target_dict=None, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[2], upsample_primary=1, use_bmuf=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=None, wandb_project='Yoruba M2M', warmup_init=False, warmup_init_lr=-1, warmup_updates=0, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': Namespace(_name='adafactor', activation_dropout=0.0, activation_fn='relu', adafactor_eps='(1e-30, 1e-3)', adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_wmt_en_de_big', attention_dropout=0.1, azureml_logging=False, batch_size=10, batch_size_valid=10, best_checkpoint_metric='loss', beta1=None, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, clip_threshold=1.0, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='data_bin', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decay_rate=-0.8, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_langtok=True, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_langtok='src', encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, eos=2, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='418M_last_checkpoint.pt', fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, lang_dict=None, lang_pairs='yo-en', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=2, lr=[3e-05], lr_period_updates=-1, lr_scheduler='cosine', lr_shrink=0.1, max_epoch=10, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=40000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=0.0, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adafactor', optimizer_overrides='{}', pad=1, patience=10, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relative_step=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='checkpoint', save_interval=1, save_interval_updates=5000, scale_parameter=False, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_dict=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, t_mult=1.0, target_dict=None, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[2], upsample_primary=1, use_bmuf=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=None, wandb_project='Yoruba M2M', warmup_init=False, warmup_init_lr=-1, warmup_updates=0, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'lr_scheduler': {'_name': 'cosine', 'warmup_updates': 0, 'warmup_init_lr': -1.0, 'lr': [3e-05], 'min_lr': 0.0, 't_mult': 1.0, 'lr_period_updates': -1.0, 'lr_shrink': 0.1, 'max_update': 40000}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}\n",
            "2021-06-25 14:27:04 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.\n",
            "2021-06-25 14:27:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['en', 'yo']\n",
            "2021-06-25 14:27:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [yo] dictionary: 128112 types\n",
            "2021-06-25 14:27:04 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 128112 types\n",
            "2021-06-25 14:27:11 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(128112, 1024, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (6): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (7): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (8): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (9): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (10): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (11): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(128112, 1024, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (6): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (7): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (8): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (9): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (10): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (11): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=1024, out_features=128112, bias=False)\n",
            "  )\n",
            ")\n",
            "2021-06-25 14:27:11 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask\n",
            "2021-06-25 14:27:11 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2021-06-25 14:27:11 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n",
            "2021-06-25 14:27:11 | INFO | fairseq_cli.train | num. shared model params: 483,905,536 (num. trained: 483,905,536)\n",
            "2021-06-25 14:27:11 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2021-06-25 14:27:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None\n",
            "2021-06-25 14:27:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3245.0234375Mb; avail=10021.2890625Mb\n",
            "2021-06-25 14:27:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
            "2021-06-25 14:27:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:yo-en': 1}\n",
            "2021-06-25 14:27:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:yo-en src_langtok: 128101; tgt_langtok: 128022\n",
            "2021-06-25 14:27:11 | INFO | fairseq.data.data_utils | loaded 100 examples from: data_bin/valid.yo-en.yo\n",
            "2021-06-25 14:27:11 | INFO | fairseq.data.data_utils | loaded 100 examples from: data_bin/valid.yo-en.en\n",
            "2021-06-25 14:27:11 | INFO | fairseq.data.multilingual.multilingual_data_manager | data_bin valid yo-en 100 examples\n",
            "2021-06-25 14:27:24 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
            "2021-06-25 14:27:24 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2021-06-25 14:27:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-06-25 14:27:24 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
            "2021-06-25 14:27:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-06-25 14:27:24 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2021-06-25 14:27:24 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 10\n",
            "2021-06-25 14:27:24 | INFO | fairseq.checkpoint_utils | loading pretrained model from 418M_last_checkpoint.pt: optimizer, lr scheduler, meters, dataloader will be reset\n",
            "2021-06-25 14:27:24 | INFO | fairseq.trainer | Preparing to load checkpoint 418M_last_checkpoint.pt\n",
            "2021-06-25 14:29:40 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2021-06-25 14:29:40 | INFO | fairseq.trainer | Loaded checkpoint 418M_last_checkpoint.pt (epoch 168 @ 0 updates)\n",
            "2021-06-25 14:29:42 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2021-06-25 14:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None\n",
            "2021-06-25 14:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8387.87890625Mb; avail=9427.1875Mb\n",
            "2021-06-25 14:29:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
            "2021-06-25 14:29:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:yo-en': 1}\n",
            "2021-06-25 14:29:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:yo-en src_langtok: 128101; tgt_langtok: 128022\n",
            "2021-06-25 14:29:42 | INFO | fairseq.data.data_utils | loaded 200 examples from: data_bin/train.yo-en.yo\n",
            "2021-06-25 14:29:42 | INFO | fairseq.data.data_utils | loaded 200 examples from: data_bin/train.yo-en.en\n",
            "2021-06-25 14:29:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | data_bin train yo-en 200 examples\n",
            "2021-06-25 14:29:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:yo-en', 200)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate\n",
            "2021-06-25 14:29:42 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat\n",
            "2021-06-25 14:29:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:yo-en': 200}; raw total size: 200\n",
            "2021-06-25 14:29:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:yo-en': 200}; resampled total size: 200\n",
            "2021-06-25 14:29:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset\n",
            "2021-06-25 14:29:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.004039\n",
            "2021-06-25 14:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=8387.46484375Mb; avail=9427.78515625Mb\n",
            "2021-06-25 14:29:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000068\n",
            "2021-06-25 14:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.005867\n",
            "2021-06-25 14:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8387.2890625Mb; avail=9427.76171875Mb\n",
            "2021-06-25 14:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000051\n",
            "2021-06-25 14:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8387.2890625Mb; avail=9427.76171875Mb\n",
            "2021-06-25 14:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.015908\n",
            "2021-06-25 14:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.022484\n",
            "2021-06-25 14:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8387.515625Mb; avail=9427.59375Mb\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaroxtn\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "2021-06-25 14:29:48.293240: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcheckpoint\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/maroxtn/Yoruba%20M2M\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/maroxtn/Yoruba%20M2M/runs/o4lewtd5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210625_142943-o4lewtd5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "2021-06-25 14:29:51 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2021-06-25 14:29:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/content/fairseq/fairseq/utils.py:366: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n",
            "2021-06-25 14:29:53 | INFO | train_inner | epoch 001:      2 / 13 loss=6.755, ppl=108.02, wps=524.3, ups=1.22, wpb=361.5, bsz=16, num_updates=2, lr=3e-05, gnorm=10.323, train_wall=2, gb_free=9.5, wall=150\n",
            "2021-06-25 14:29:55 | INFO | train_inner | epoch 001:      4 / 13 loss=6.166, ppl=71.78, wps=469.5, ups=1.09, wpb=429.5, bsz=16, num_updates=4, lr=3e-05, gnorm=9.323, train_wall=2, gb_free=9.2, wall=151\n",
            "2021-06-25 14:29:57 | INFO | train_inner | epoch 001:      6 / 13 loss=6.403, ppl=84.61, wps=508.1, ups=1.08, wpb=470.5, bsz=16, num_updates=6, lr=3e-05, gnorm=9.619, train_wall=2, gb_free=9.4, wall=153\n",
            "2021-06-25 14:29:58 | INFO | train_inner | epoch 001:      8 / 13 loss=6.29, ppl=78.25, wps=477.3, ups=1.33, wpb=358.5, bsz=16, num_updates=8, lr=3e-05, gnorm=10.243, train_wall=1, gb_free=9.6, wall=155\n",
            "2021-06-25 14:30:00 | INFO | train_inner | epoch 001:     10 / 13 loss=6.036, ppl=65.64, wps=551.7, ups=0.95, wpb=579, bsz=16, num_updates=10, lr=3e-05, gnorm=8.802, train_wall=2, gb_free=9.1, wall=157\n",
            "2021-06-25 14:30:03 | INFO | train_inner | epoch 001:     12 / 13 loss=6.117, ppl=69.4, wps=526.5, ups=0.92, wpb=575, bsz=16, num_updates=12, lr=3e-05, gnorm=7.979, train_wall=2, gb_free=9.2, wall=159\n",
            "2021-06-25 14:30:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-06-25 14:30:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=8355.90234375Mb; avail=9214.42578125Mb\n",
            "2021-06-25 14:30:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001113\n",
            "2021-06-25 14:30:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8355.90234375Mb; avail=9214.42578125Mb\n",
            "2021-06-25 14:30:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.000857\n",
            "2021-06-25 14:30:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8355.90234375Mb; avail=9214.42578125Mb\n",
            "2021-06-25 14:30:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.000831\n",
            "2021-06-25 14:30:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.003662\n",
            "2021-06-25 14:30:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8355.90234375Mb; avail=9214.42578125Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:30:05 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 5.326 | ppl 40.12 | wps 1794.9 | wpb 203.6 | bsz 7.7 | num_updates 13\n",
            "2021-06-25 14:30:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 13 updates\n",
            "2021-06-25 14:30:05 | INFO | fairseq.trainer | Saving checkpoint to checkpoint/checkpoint_best.pt\n",
            "2021-06-25 14:30:53 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoint/checkpoint_best.pt\n",
            "Removed checkpoint_last.pt\n",
            "2021-06-25 14:32:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint/checkpoint_best.pt (epoch 1 @ 13 updates, score 5.326) (writing took 136.95247700800002 seconds)\n",
            "2021-06-25 14:32:22 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2021-06-25 14:32:22 | INFO | train | epoch 001 | loss 6.275 | ppl 77.45 | wps 36.6 | ups 0.08 | wpb 443.2 | bsz 15.4 | num_updates 13 | lr 3e-05 | gnorm 9.504 | train_wall 12 | gb_free 10.2 | wall 298\n",
            "2021-06-25 14:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=3554.37890625Mb; avail=9122.4140625Mb\n",
            "2021-06-25 14:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000886\n",
            "2021-06-25 14:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3554.33203125Mb; avail=9122.57421875Mb\n",
            "2021-06-25 14:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.005463\n",
            "2021-06-25 14:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3554.33984375Mb; avail=9122.55078125Mb\n",
            "2021-06-25 14:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000299\n",
            "2021-06-25 14:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.007776\n",
            "2021-06-25 14:32:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3554.33984375Mb; avail=9122.55078125Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:32:22 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2021-06-25 14:32:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2021-06-25 14:32:23 | INFO | train_inner | epoch 002:      1 / 13 loss=6.102, ppl=68.7, wps=5.6, ups=0.01, wpb=396.5, bsz=12, num_updates=14, lr=3e-05, gnorm=8.881, train_wall=2, gb_free=9.1, wall=299\n",
            "2021-06-25 14:32:25 | INFO | train_inner | epoch 002:      3 / 13 loss=5.045, ppl=33.02, wps=485, ups=1.04, wpb=464.5, bsz=16, num_updates=16, lr=3e-05, gnorm=6.827, train_wall=2, gb_free=9.2, wall=301\n",
            "2021-06-25 14:32:27 | INFO | train_inner | epoch 002:      5 / 13 loss=5.305, ppl=39.53, wps=467.7, ups=1.13, wpb=413.5, bsz=16, num_updates=18, lr=3e-05, gnorm=8.99, train_wall=2, gb_free=8, wall=303\n",
            "2021-06-25 14:32:28 | INFO | train_inner | epoch 002:      7 / 13 loss=5.104, ppl=34.39, wps=487.8, ups=1.13, wpb=430.5, bsz=16, num_updates=20, lr=3e-05, gnorm=7.8, train_wall=2, gb_free=9.5, wall=305\n",
            "2021-06-25 14:32:30 | INFO | train_inner | epoch 002:      9 / 13 loss=5.395, ppl=42.07, wps=487.8, ups=1.13, wpb=430, bsz=16, num_updates=22, lr=3e-05, gnorm=8.585, train_wall=2, gb_free=9.4, wall=307\n",
            "2021-06-25 14:32:32 | INFO | train_inner | epoch 002:     11 / 13 loss=5.737, ppl=53.33, wps=524.2, ups=0.92, wpb=571.5, bsz=16, num_updates=24, lr=3e-05, gnorm=7.566, train_wall=2, gb_free=9.6, wall=309\n",
            "2021-06-25 14:32:34 | INFO | train_inner | epoch 002:     13 / 13 loss=5.521, ppl=45.93, wps=462, ups=1.64, wpb=281.5, bsz=12, num_updates=26, lr=3e-05, gnorm=9.971, train_wall=1, gb_free=10.2, wall=310\n",
            "2021-06-25 14:32:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-06-25 14:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=3506.7734375Mb; avail=9170.44921875Mb\n",
            "2021-06-25 14:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001353\n",
            "2021-06-25 14:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3506.7734375Mb; avail=9170.44921875Mb\n",
            "2021-06-25 14:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.001380\n",
            "2021-06-25 14:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3506.7734375Mb; avail=9170.44921875Mb\n",
            "2021-06-25 14:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001090\n",
            "2021-06-25 14:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004877\n",
            "2021-06-25 14:32:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3506.7734375Mb; avail=9170.44921875Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:32:35 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.195 | ppl 36.63 | wps 1760.6 | wpb 203.6 | bsz 7.7 | num_updates 26 | best_loss 5.195\n",
            "2021-06-25 14:32:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 26 updates\n",
            "2021-06-25 14:32:35 | INFO | fairseq.trainer | Saving checkpoint to checkpoint/checkpoint_best.pt\n",
            "2021-06-25 14:33:23 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoint/checkpoint_best.pt\n",
            "Removed checkpoint_last.pt\n",
            "2021-06-25 14:34:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint/checkpoint_best.pt (epoch 2 @ 26 updates, score 5.195) (writing took 135.50153005099992 seconds)\n",
            "2021-06-25 14:34:51 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2021-06-25 14:34:51 | INFO | train | epoch 002 | loss 5.414 | ppl 42.63 | wps 38.7 | ups 0.09 | wpb 443.2 | bsz 15.4 | num_updates 26 | lr 3e-05 | gnorm 8.174 | train_wall 12 | gb_free 10.2 | wall 447\n",
            "2021-06-25 14:34:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=3510.10546875Mb; avail=9159.0234375Mb\n",
            "2021-06-25 14:34:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000852\n",
            "2021-06-25 14:34:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3510.10546875Mb; avail=9159.0234375Mb\n",
            "2021-06-25 14:34:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000075\n",
            "2021-06-25 14:34:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3510.10546875Mb; avail=9159.0234375Mb\n",
            "2021-06-25 14:34:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000293\n",
            "2021-06-25 14:34:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002029\n",
            "2021-06-25 14:34:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3510.10546875Mb; avail=9158.9921875Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:34:51 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2021-06-25 14:34:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2021-06-25 14:34:53 | INFO | train_inner | epoch 003:      2 / 13 loss=4.53, ppl=23.11, wps=6.8, ups=0.01, wpb=470, bsz=16, num_updates=28, lr=3e-05, gnorm=8.244, train_wall=2, gb_free=9.5, wall=449\n",
            "2021-06-25 14:34:54 | INFO | train_inner | epoch 003:      4 / 13 loss=4.724, ppl=26.43, wps=459.2, ups=1.29, wpb=355, bsz=16, num_updates=30, lr=3e-05, gnorm=8.44, train_wall=2, gb_free=9.4, wall=451\n",
            "2021-06-25 14:34:56 | INFO | train_inner | epoch 003:      6 / 13 loss=5.229, ppl=37.51, wps=532.8, ups=1.17, wpb=457, bsz=16, num_updates=32, lr=3e-05, gnorm=8.676, train_wall=2, gb_free=9.1, wall=452\n",
            "2021-06-25 14:34:58 | INFO | train_inner | epoch 003:      8 / 13 loss=5.088, ppl=34.02, wps=528.8, ups=0.88, wpb=601, bsz=16, num_updates=34, lr=2.99999e-05, gnorm=7.958, train_wall=2, gb_free=8.7, wall=455\n",
            "2021-06-25 14:35:00 | INFO | train_inner | epoch 003:     10 / 13 loss=4.855, ppl=28.93, wps=508.6, ups=1.02, wpb=497, bsz=16, num_updates=36, lr=2.99999e-05, gnorm=7.283, train_wall=2, gb_free=8.3, wall=457\n",
            "2021-06-25 14:35:02 | INFO | train_inner | epoch 003:     12 / 13 loss=5.182, ppl=36.3, wps=450.4, ups=1.17, wpb=383.5, bsz=16, num_updates=38, lr=2.99999e-05, gnorm=8.29, train_wall=2, gb_free=8.6, wall=458\n",
            "2021-06-25 14:35:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-06-25 14:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=3506.15234375Mb; avail=9164.10546875Mb\n",
            "2021-06-25 14:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001067\n",
            "2021-06-25 14:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3506.15234375Mb; avail=9164.10546875Mb\n",
            "2021-06-25 14:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.001020\n",
            "2021-06-25 14:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3506.15234375Mb; avail=9164.10546875Mb\n",
            "2021-06-25 14:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.000822\n",
            "2021-06-25 14:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.003988\n",
            "2021-06-25 14:35:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=3506.15234375Mb; avail=9164.10546875Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:35:04 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.217 | ppl 37.2 | wps 1750 | wpb 203.6 | bsz 7.7 | num_updates 39 | best_loss 5.195\n",
            "2021-06-25 14:35:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 39 updates\n",
            "2021-06-25 14:35:04 | INFO | fairseq.trainer | Saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:35:51 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:35:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint/checkpoint_last.pt (epoch 3 @ 39 updates, score 5.217) (writing took 47.56589175900001 seconds)\n",
            "2021-06-25 14:35:52 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2021-06-25 14:35:52 | INFO | train | epoch 003 | loss 4.921 | ppl 30.3 | wps 94.4 | ups 0.21 | wpb 443.2 | bsz 15.4 | num_updates 39 | lr 2.99999e-05 | gnorm 8.233 | train_wall 12 | gb_free 10.1 | wall 508\n",
            "2021-06-25 14:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6865.48828125Mb; avail=9156.62890625Mb\n",
            "2021-06-25 14:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000915\n",
            "2021-06-25 14:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6865.48828125Mb; avail=9156.62890625Mb\n",
            "2021-06-25 14:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000077\n",
            "2021-06-25 14:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6865.48828125Mb; avail=9156.62890625Mb\n",
            "2021-06-25 14:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000349\n",
            "2021-06-25 14:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002302\n",
            "2021-06-25 14:35:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6865.48828125Mb; avail=9156.62890625Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:35:53 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2021-06-25 14:35:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2021-06-25 14:35:54 | INFO | train_inner | epoch 004:      1 / 13 loss=4.603, ppl=24.3, wps=17, ups=0.04, wpb=444.5, bsz=12, num_updates=40, lr=2.99999e-05, gnorm=8.023, train_wall=2, gb_free=9.1, wall=511\n",
            "Removed checkpoint_last.pt\n",
            "2021-06-25 14:35:56 | INFO | train_inner | epoch 004:      3 / 13 loss=4.541, ppl=23.28, wps=523.5, ups=1.1, wpb=476.5, bsz=16, num_updates=42, lr=2.99999e-05, gnorm=7.391, train_wall=2, gb_free=9.1, wall=512\n",
            "2021-06-25 14:35:58 | INFO | train_inner | epoch 004:      5 / 13 loss=4.87, ppl=29.23, wps=512.5, ups=0.82, wpb=625.5, bsz=16, num_updates=44, lr=2.99999e-05, gnorm=6.886, train_wall=2, gb_free=9.5, wall=515\n",
            "2021-06-25 14:36:00 | INFO | train_inner | epoch 004:      7 / 13 loss=4.342, ppl=20.29, wps=454.8, ups=1.21, wpb=377, bsz=16, num_updates=46, lr=2.99999e-05, gnorm=8.635, train_wall=2, gb_free=9.6, wall=517\n",
            "2021-06-25 14:36:02 | INFO | train_inner | epoch 004:      9 / 13 loss=4.544, ppl=23.33, wps=445.9, ups=1.12, wpb=399, bsz=16, num_updates=48, lr=2.99999e-05, gnorm=7.835, train_wall=2, gb_free=9.5, wall=518\n",
            "2021-06-25 14:36:04 | INFO | train_inner | epoch 004:     11 / 13 loss=4.282, ppl=19.45, wps=474.6, ups=1.23, wpb=385.5, bsz=16, num_updates=50, lr=2.99999e-05, gnorm=7.905, train_wall=2, gb_free=9.5, wall=520\n",
            "2021-06-25 14:36:05 | INFO | train_inner | epoch 004:     13 / 13 loss=3.941, ppl=15.36, wps=422.8, ups=1.46, wpb=290.5, bsz=12, num_updates=52, lr=2.99999e-05, gnorm=9.195, train_wall=1, gb_free=10.2, wall=521\n",
            "2021-06-25 14:36:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-06-25 14:36:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6858.95703125Mb; avail=9163.5078125Mb\n",
            "2021-06-25 14:36:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001987\n",
            "2021-06-25 14:36:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6858.95703125Mb; avail=9163.5078125Mb\n",
            "2021-06-25 14:36:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.001690\n",
            "2021-06-25 14:36:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6858.95703125Mb; avail=9163.5078125Mb\n",
            "2021-06-25 14:36:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.000903\n",
            "2021-06-25 14:36:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.005586\n",
            "2021-06-25 14:36:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6858.95703125Mb; avail=9163.5078125Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:36:07 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.223 | ppl 37.36 | wps 1734.9 | wpb 203.6 | bsz 7.7 | num_updates 52 | best_loss 5.195\n",
            "2021-06-25 14:36:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 52 updates\n",
            "2021-06-25 14:36:07 | INFO | fairseq.trainer | Saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:37:05 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:37:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint/checkpoint_last.pt (epoch 4 @ 52 updates, score 5.223) (writing took 59.09230469500005 seconds)\n",
            "2021-06-25 14:37:06 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2021-06-25 14:37:06 | INFO | train | epoch 004 | loss 4.505 | ppl 22.7 | wps 77.9 | ups 0.18 | wpb 443.2 | bsz 15.4 | num_updates 52 | lr 2.99999e-05 | gnorm 7.884 | train_wall 12 | gb_free 10.2 | wall 582\n",
            "2021-06-25 14:37:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6866.47265625Mb; avail=9156.26953125Mb\n",
            "2021-06-25 14:37:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000814\n",
            "2021-06-25 14:37:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6866.47265625Mb; avail=9156.26953125Mb\n",
            "2021-06-25 14:37:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000058\n",
            "2021-06-25 14:37:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6866.47265625Mb; avail=9156.26953125Mb\n",
            "2021-06-25 14:37:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000286\n",
            "2021-06-25 14:37:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001949\n",
            "2021-06-25 14:37:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6866.47265625Mb; avail=9156.26953125Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:37:08 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2021-06-25 14:37:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "Removed checkpoint_last.pt\n",
            "2021-06-25 14:37:10 | INFO | train_inner | epoch 005:      2 / 13 loss=4.066, ppl=16.75, wps=13, ups=0.03, wpb=424.5, bsz=16, num_updates=54, lr=2.99999e-05, gnorm=7.78, train_wall=2, gb_free=9.5, wall=587\n",
            "2021-06-25 14:37:12 | INFO | train_inner | epoch 005:      4 / 13 loss=3.888, ppl=14.81, wps=409.1, ups=1.52, wpb=268.5, bsz=16, num_updates=56, lr=2.99999e-05, gnorm=9.024, train_wall=1, gb_free=9.4, wall=588\n",
            "2021-06-25 14:37:14 | INFO | train_inner | epoch 005:      6 / 13 loss=4.665, ppl=25.37, wps=561, ups=1.02, wpb=548.5, bsz=16, num_updates=58, lr=2.99998e-05, gnorm=7.67, train_wall=2, gb_free=9.1, wall=590\n",
            "2021-06-25 14:37:16 | INFO | train_inner | epoch 005:      8 / 13 loss=4.154, ppl=17.8, wps=429.2, ups=1.04, wpb=414.5, bsz=16, num_updates=60, lr=2.99998e-05, gnorm=8.118, train_wall=2, gb_free=9.3, wall=592\n",
            "2021-06-25 14:37:17 | INFO | train_inner | epoch 005:     10 / 13 loss=3.87, ppl=14.62, wps=516.6, ups=1.02, wpb=508.5, bsz=16, num_updates=62, lr=2.99998e-05, gnorm=7.24, train_wall=2, gb_free=9.3, wall=594\n",
            "2021-06-25 14:37:20 | INFO | train_inner | epoch 005:     12 / 13 loss=4.19, ppl=18.25, wps=510.6, ups=0.84, wpb=608, bsz=16, num_updates=64, lr=2.99998e-05, gnorm=7.98, train_wall=2, gb_free=9.4, wall=596\n",
            "2021-06-25 14:37:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-06-25 14:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6858.87890625Mb; avail=9163.89453125Mb\n",
            "2021-06-25 14:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001114\n",
            "2021-06-25 14:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6859.125Mb; avail=9163.6484375Mb\n",
            "2021-06-25 14:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.000846\n",
            "2021-06-25 14:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6859.125Mb; avail=9163.6484375Mb\n",
            "2021-06-25 14:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.000814\n",
            "2021-06-25 14:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.003799\n",
            "2021-06-25 14:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6859.125Mb; avail=9163.6484375Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:37:22 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.222 | ppl 37.33 | wps 1723.6 | wpb 203.6 | bsz 7.7 | num_updates 65 | best_loss 5.195\n",
            "2021-06-25 14:37:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 65 updates\n",
            "2021-06-25 14:37:22 | INFO | fairseq.trainer | Saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:38:20 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:38:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint/checkpoint_last.pt (epoch 5 @ 65 updates, score 5.222) (writing took 58.36147870299999 seconds)\n",
            "2021-06-25 14:38:20 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2021-06-25 14:38:20 | INFO | train | epoch 005 | loss 4.16 | ppl 17.88 | wps 77 | ups 0.17 | wpb 443.2 | bsz 15.4 | num_updates 65 | lr 2.99998e-05 | gnorm 8.197 | train_wall 12 | gb_free 10.2 | wall 657\n",
            "2021-06-25 14:38:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6873.69140625Mb; avail=9152.1875Mb\n",
            "2021-06-25 14:38:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000848\n",
            "2021-06-25 14:38:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6873.69140625Mb; avail=9152.1875Mb\n",
            "2021-06-25 14:38:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000059\n",
            "2021-06-25 14:38:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6873.69140625Mb; avail=9152.1875Mb\n",
            "2021-06-25 14:38:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000295\n",
            "2021-06-25 14:38:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001994\n",
            "2021-06-25 14:38:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6873.69140625Mb; avail=9152.1875Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:38:21 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2021-06-25 14:38:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2021-06-25 14:38:21 | INFO | train_inner | epoch 006:      1 / 13 loss=4.027, ppl=16.3, wps=7.5, ups=0.03, wpb=231.5, bsz=12, num_updates=66, lr=2.99998e-05, gnorm=10.036, train_wall=1, gb_free=9.5, wall=658\n",
            "2021-06-25 14:38:23 | INFO | train_inner | epoch 006:      3 / 13 loss=3.694, ppl=12.95, wps=489.2, ups=1.15, wpb=426.5, bsz=16, num_updates=68, lr=2.99998e-05, gnorm=7.013, train_wall=2, gb_free=8.9, wall=660\n",
            "Removed checkpoint_last.pt\n",
            "2021-06-25 14:38:25 | INFO | train_inner | epoch 006:      5 / 13 loss=3.335, ppl=10.09, wps=426.8, ups=1.33, wpb=320.5, bsz=16, num_updates=70, lr=2.99998e-05, gnorm=8.253, train_wall=1, gb_free=9.6, wall=661\n",
            "2021-06-25 14:38:27 | INFO | train_inner | epoch 006:      7 / 13 loss=3.888, ppl=14.8, wps=501.1, ups=0.89, wpb=560.5, bsz=16, num_updates=72, lr=2.99998e-05, gnorm=6.913, train_wall=2, gb_free=9.1, wall=663\n",
            "2021-06-25 14:38:29 | INFO | train_inner | epoch 006:      9 / 13 loss=4.187, ppl=18.21, wps=541.7, ups=0.87, wpb=620.5, bsz=16, num_updates=74, lr=2.99997e-05, gnorm=6.974, train_wall=2, gb_free=9.4, wall=666\n",
            "2021-06-25 14:38:31 | INFO | train_inner | epoch 006:     11 / 13 loss=3.714, ppl=13.12, wps=494.8, ups=1.12, wpb=443, bsz=16, num_updates=76, lr=2.99997e-05, gnorm=7.371, train_wall=2, gb_free=9.5, wall=667\n",
            "2021-06-25 14:38:33 | INFO | train_inner | epoch 006:     13 / 13 loss=3.511, ppl=11.4, wps=465, ups=1.2, wpb=387, bsz=12, num_updates=78, lr=2.99997e-05, gnorm=7.814, train_wall=2, gb_free=10.1, wall=669\n",
            "2021-06-25 14:38:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-06-25 14:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6861.77734375Mb; avail=9160.54296875Mb\n",
            "2021-06-25 14:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001381\n",
            "2021-06-25 14:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6862.0234375Mb; avail=9160.296875Mb\n",
            "2021-06-25 14:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.001409\n",
            "2021-06-25 14:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6862.0234375Mb; avail=9160.296875Mb\n",
            "2021-06-25 14:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.000970\n",
            "2021-06-25 14:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004771\n",
            "2021-06-25 14:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6862.0234375Mb; avail=9160.296875Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:38:34 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.237 | ppl 37.72 | wps 1723.2 | wpb 203.6 | bsz 7.7 | num_updates 78 | best_loss 5.195\n",
            "2021-06-25 14:38:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 78 updates\n",
            "2021-06-25 14:38:34 | INFO | fairseq.trainer | Saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:39:34 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:39:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint/checkpoint_last.pt (epoch 6 @ 78 updates, score 5.237) (writing took 60.511054099000035 seconds)\n",
            "2021-06-25 14:39:35 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2021-06-25 14:39:35 | INFO | train | epoch 006 | loss 3.797 | ppl 13.9 | wps 77.4 | ups 0.17 | wpb 443.2 | bsz 15.4 | num_updates 78 | lr 2.99997e-05 | gnorm 7.524 | train_wall 12 | gb_free 10.1 | wall 731\n",
            "2021-06-25 14:39:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6902.96875Mb; avail=9118.27734375Mb\n",
            "2021-06-25 14:39:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000854\n",
            "2021-06-25 14:39:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6902.96875Mb; avail=9118.27734375Mb\n",
            "2021-06-25 14:39:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000066\n",
            "2021-06-25 14:39:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6902.96875Mb; avail=9118.27734375Mb\n",
            "2021-06-25 14:39:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000323\n",
            "2021-06-25 14:39:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002094\n",
            "2021-06-25 14:39:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6902.96875Mb; avail=9118.27734375Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:39:36 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2021-06-25 14:39:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2021-06-25 14:39:38 | INFO | train_inner | epoch 007:      2 / 13 loss=3.534, ppl=11.58, wps=16.5, ups=0.03, wpb=540, bsz=16, num_updates=80, lr=2.99997e-05, gnorm=7.454, train_wall=2, gb_free=8, wall=735\n",
            "Removed checkpoint_last.pt\n",
            "2021-06-25 14:39:40 | INFO | train_inner | epoch 007:      4 / 13 loss=2.801, ppl=6.97, wps=398.4, ups=1.43, wpb=279, bsz=16, num_updates=82, lr=2.99997e-05, gnorm=8.007, train_wall=1, gb_free=9.5, wall=736\n",
            "2021-06-25 14:39:41 | INFO | train_inner | epoch 007:      6 / 13 loss=3.424, ppl=10.73, wps=490.2, ups=1.15, wpb=425.5, bsz=16, num_updates=84, lr=2.99997e-05, gnorm=8.289, train_wall=2, gb_free=9.4, wall=738\n",
            "2021-06-25 14:39:44 | INFO | train_inner | epoch 007:      8 / 13 loss=3.832, ppl=14.24, wps=463.4, ups=0.85, wpb=545.5, bsz=16, num_updates=86, lr=2.99997e-05, gnorm=7.151, train_wall=2, gb_free=8.6, wall=740\n",
            "2021-06-25 14:39:46 | INFO | train_inner | epoch 007:     10 / 13 loss=3.644, ppl=12.5, wps=550.5, ups=1.05, wpb=523, bsz=16, num_updates=88, lr=2.99996e-05, gnorm=6.996, train_wall=2, gb_free=9.4, wall=742\n",
            "2021-06-25 14:39:48 | INFO | train_inner | epoch 007:     12 / 13 loss=3.715, ppl=13.13, wps=514.7, ups=1.03, wpb=502, bsz=16, num_updates=90, lr=2.99996e-05, gnorm=7.199, train_wall=2, gb_free=8.3, wall=744\n",
            "2021-06-25 14:39:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-06-25 14:39:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6861.1015625Mb; avail=9161.40625Mb\n",
            "2021-06-25 14:39:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001181\n",
            "2021-06-25 14:39:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6861.1015625Mb; avail=9161.40625Mb\n",
            "2021-06-25 14:39:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.000897\n",
            "2021-06-25 14:39:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6861.1015625Mb; avail=9161.40625Mb\n",
            "2021-06-25 14:39:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.000881\n",
            "2021-06-25 14:39:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.003815\n",
            "2021-06-25 14:39:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6861.1015625Mb; avail=9161.40625Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:39:50 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.247 | ppl 37.97 | wps 1712.8 | wpb 203.6 | bsz 7.7 | num_updates 91 | best_loss 5.195\n",
            "2021-06-25 14:39:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 91 updates\n",
            "2021-06-25 14:39:50 | INFO | fairseq.trainer | Saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:40:47 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:40:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint/checkpoint_last.pt (epoch 7 @ 91 updates, score 5.247) (writing took 58.214364805999935 seconds)\n",
            "2021-06-25 14:40:48 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2021-06-25 14:40:48 | INFO | train | epoch 007 | loss 3.533 | ppl 11.57 | wps 78.9 | ups 0.18 | wpb 443.2 | bsz 15.4 | num_updates 91 | lr 2.99996e-05 | gnorm 7.782 | train_wall 12 | gb_free 10.2 | wall 804\n",
            "2021-06-25 14:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6866.35546875Mb; avail=9154.75Mb\n",
            "2021-06-25 14:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000876\n",
            "2021-06-25 14:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6866.35546875Mb; avail=9154.75Mb\n",
            "2021-06-25 14:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000064\n",
            "2021-06-25 14:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6866.35546875Mb; avail=9154.75Mb\n",
            "2021-06-25 14:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.001433\n",
            "2021-06-25 14:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.004153\n",
            "2021-06-25 14:40:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6866.6953125Mb; avail=9154.41015625Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:40:51 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2021-06-25 14:40:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "Removed checkpoint_last.pt\n",
            "2021-06-25 14:40:52 | INFO | train_inner | epoch 008:      1 / 13 loss=2.934, ppl=7.64, wps=7.1, ups=0.03, wpb=228.5, bsz=12, num_updates=92, lr=2.99996e-05, gnorm=9.55, train_wall=1, gb_free=9.4, wall=808\n",
            "2021-06-25 14:40:53 | INFO | train_inner | epoch 008:      3 / 13 loss=3.327, ppl=10.03, wps=472.5, ups=1.32, wpb=359, bsz=16, num_updates=94, lr=2.99996e-05, gnorm=9.043, train_wall=2, gb_free=9.1, wall=810\n",
            "2021-06-25 14:40:55 | INFO | train_inner | epoch 008:      5 / 13 loss=3.038, ppl=8.22, wps=457.1, ups=1.3, wpb=351, bsz=16, num_updates=96, lr=2.99996e-05, gnorm=7.402, train_wall=2, gb_free=9.3, wall=811\n",
            "2021-06-25 14:40:57 | INFO | train_inner | epoch 008:      7 / 13 loss=3.063, ppl=8.36, wps=523.4, ups=1.06, wpb=496, bsz=16, num_updates=98, lr=2.99996e-05, gnorm=6.43, train_wall=2, gb_free=9.4, wall=813\n",
            "2021-06-25 14:40:59 | INFO | train_inner | epoch 008:      9 / 13 loss=3.29, ppl=9.78, wps=467.8, ups=0.95, wpb=490.5, bsz=16, num_updates=100, lr=2.99995e-05, gnorm=6.835, train_wall=2, gb_free=6.9, wall=815\n",
            "2021-06-25 14:41:01 | INFO | train_inner | epoch 008:     11 / 13 loss=3.521, ppl=11.48, wps=527.1, ups=1.07, wpb=493, bsz=16, num_updates=102, lr=2.99995e-05, gnorm=8.585, train_wall=2, gb_free=9.6, wall=817\n",
            "2021-06-25 14:41:03 | INFO | train_inner | epoch 008:     13 / 13 loss=3.152, ppl=8.89, wps=479.1, ups=0.91, wpb=529, bsz=12, num_updates=104, lr=2.99995e-05, gnorm=7.103, train_wall=2, gb_free=10, wall=819\n",
            "2021-06-25 14:41:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-06-25 14:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6859.61328125Mb; avail=9162.88671875Mb\n",
            "2021-06-25 14:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001100\n",
            "2021-06-25 14:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6859.61328125Mb; avail=9162.88671875Mb\n",
            "2021-06-25 14:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.000910\n",
            "2021-06-25 14:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6859.61328125Mb; avail=9162.88671875Mb\n",
            "2021-06-25 14:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.000815\n",
            "2021-06-25 14:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.003833\n",
            "2021-06-25 14:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6859.61328125Mb; avail=9162.88671875Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:41:04 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.424 | ppl 42.94 | wps 1720.5 | wpb 203.6 | bsz 7.7 | num_updates 104 | best_loss 5.195\n",
            "2021-06-25 14:41:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 104 updates\n",
            "2021-06-25 14:41:05 | INFO | fairseq.trainer | Saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:42:03 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:42:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint/checkpoint_last.pt (epoch 8 @ 104 updates, score 5.424) (writing took 58.86263268899984 seconds)\n",
            "2021-06-25 14:42:03 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2021-06-25 14:42:03 | INFO | train | epoch 008 | loss 3.227 | ppl 9.36 | wps 76.3 | ups 0.17 | wpb 443.2 | bsz 15.4 | num_updates 104 | lr 2.99995e-05 | gnorm 7.61 | train_wall 12 | gb_free 10 | wall 880\n",
            "2021-06-25 14:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6870.92578125Mb; avail=9152.06640625Mb\n",
            "2021-06-25 14:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000834\n",
            "2021-06-25 14:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6870.92578125Mb; avail=9152.06640625Mb\n",
            "2021-06-25 14:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069\n",
            "2021-06-25 14:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6870.92578125Mb; avail=9152.06640625Mb\n",
            "2021-06-25 14:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000306\n",
            "2021-06-25 14:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002050\n",
            "2021-06-25 14:42:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6870.92578125Mb; avail=9152.06640625Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Removed checkpoint_last.pt\n",
            "2021-06-25 14:42:05 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2021-06-25 14:42:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2021-06-25 14:42:06 | INFO | train_inner | epoch 009:      2 / 13 loss=3.039, ppl=8.22, wps=9, ups=0.03, wpb=283.5, bsz=16, num_updates=106, lr=2.99995e-05, gnorm=8.534, train_wall=1, gb_free=9.5, wall=883\n",
            "2021-06-25 14:42:08 | INFO | train_inner | epoch 009:      4 / 13 loss=2.866, ppl=7.29, wps=492, ups=1.07, wpb=459, bsz=16, num_updates=108, lr=2.99995e-05, gnorm=7.112, train_wall=2, gb_free=9.5, wall=884\n",
            "2021-06-25 14:42:10 | INFO | train_inner | epoch 009:      6 / 13 loss=2.948, ppl=7.72, wps=474.5, ups=0.91, wpb=521, bsz=16, num_updates=110, lr=2.99994e-05, gnorm=6.665, train_wall=2, gb_free=8.6, wall=887\n",
            "2021-06-25 14:42:12 | INFO | train_inner | epoch 009:      8 / 13 loss=2.747, ppl=6.71, wps=517.7, ups=1.06, wpb=489.5, bsz=16, num_updates=112, lr=2.99994e-05, gnorm=6.599, train_wall=2, gb_free=8.9, wall=889\n",
            "2021-06-25 14:42:15 | INFO | train_inner | epoch 009:     10 / 13 loss=3.169, ppl=8.99, wps=541, ups=0.79, wpb=683, bsz=16, num_updates=114, lr=2.99994e-05, gnorm=6.093, train_wall=3, gb_free=9.2, wall=891\n",
            "2021-06-25 14:42:16 | INFO | train_inner | epoch 009:     12 / 13 loss=3.087, ppl=8.5, wps=462, ups=1.29, wpb=357, bsz=16, num_updates=116, lr=2.99994e-05, gnorm=9.801, train_wall=2, gb_free=9.5, wall=893\n",
            "2021-06-25 14:42:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-06-25 14:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6859.6640625Mb; avail=9161.55859375Mb\n",
            "2021-06-25 14:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001092\n",
            "2021-06-25 14:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6859.6640625Mb; avail=9161.55859375Mb\n",
            "2021-06-25 14:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.000866\n",
            "2021-06-25 14:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6859.6640625Mb; avail=9161.55859375Mb\n",
            "2021-06-25 14:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.000839\n",
            "2021-06-25 14:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.003720\n",
            "2021-06-25 14:42:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6859.6640625Mb; avail=9161.55859375Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:42:18 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.385 | ppl 41.78 | wps 1703.7 | wpb 203.6 | bsz 7.7 | num_updates 117 | best_loss 5.195\n",
            "2021-06-25 14:42:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 117 updates\n",
            "2021-06-25 14:42:18 | INFO | fairseq.trainer | Saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:43:15 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:43:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint/checkpoint_last.pt (epoch 9 @ 117 updates, score 5.385) (writing took 57.64957751599991 seconds)\n",
            "2021-06-25 14:43:16 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2021-06-25 14:43:16 | INFO | train | epoch 009 | loss 2.976 | ppl 7.87 | wps 79.4 | ups 0.18 | wpb 443.2 | bsz 15.4 | num_updates 117 | lr 2.99994e-05 | gnorm 7.713 | train_wall 12 | gb_free 10.1 | wall 953\n",
            "2021-06-25 14:43:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6869.01953125Mb; avail=9151.4765625Mb\n",
            "2021-06-25 14:43:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000812\n",
            "2021-06-25 14:43:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6869.01953125Mb; avail=9151.4765625Mb\n",
            "2021-06-25 14:43:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000065\n",
            "2021-06-25 14:43:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6869.01953125Mb; avail=9151.4765625Mb\n",
            "2021-06-25 14:43:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000299\n",
            "2021-06-25 14:43:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002011\n",
            "2021-06-25 14:43:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6869.01953125Mb; avail=9151.4765625Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:43:19 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2021-06-25 14:43:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "Removed checkpoint_last.pt\n",
            "2021-06-25 14:43:20 | INFO | train_inner | epoch 010:      1 / 13 loss=2.949, ppl=7.72, wps=11.9, ups=0.03, wpb=381, bsz=12, num_updates=118, lr=2.99994e-05, gnorm=8.511, train_wall=2, gb_free=8.8, wall=957\n",
            "2021-06-25 14:43:22 | INFO | train_inner | epoch 010:      3 / 13 loss=2.145, ppl=4.42, wps=428.2, ups=1.26, wpb=340, bsz=16, num_updates=120, lr=2.99993e-05, gnorm=7.008, train_wall=2, gb_free=9.4, wall=958\n",
            "2021-06-25 14:43:24 | INFO | train_inner | epoch 010:      5 / 13 loss=2.808, ppl=7, wps=517.2, ups=1.1, wpb=470, bsz=16, num_updates=122, lr=2.99993e-05, gnorm=7.489, train_wall=2, gb_free=8.3, wall=960\n",
            "2021-06-25 14:43:25 | INFO | train_inner | epoch 010:      7 / 13 loss=2.355, ppl=5.12, wps=475.4, ups=1.18, wpb=402.5, bsz=16, num_updates=124, lr=2.99993e-05, gnorm=8.11, train_wall=2, gb_free=9.2, wall=962\n",
            "2021-06-25 14:43:28 | INFO | train_inner | epoch 010:      9 / 13 loss=3.131, ppl=8.76, wps=481.5, ups=0.74, wpb=650, bsz=16, num_updates=126, lr=2.99993e-05, gnorm=7.042, train_wall=3, gb_free=8, wall=964\n",
            "2021-06-25 14:43:30 | INFO | train_inner | epoch 010:     11 / 13 loss=2.773, ppl=6.84, wps=438.6, ups=1.26, wpb=347, bsz=16, num_updates=128, lr=2.99992e-05, gnorm=7.837, train_wall=2, gb_free=9.4, wall=966\n",
            "2021-06-25 14:43:31 | INFO | train_inner | epoch 010:     13 / 13 loss=2.893, ppl=7.43, wps=492.7, ups=1.3, wpb=378.5, bsz=12, num_updates=130, lr=2.99992e-05, gnorm=7.233, train_wall=2, gb_free=10.1, wall=968\n",
            "2021-06-25 14:43:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2021-06-25 14:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6861.3125Mb; avail=9159.5234375Mb\n",
            "2021-06-25 14:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001295\n",
            "2021-06-25 14:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6861.3125Mb; avail=9159.5234375Mb\n",
            "2021-06-25 14:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.001412\n",
            "2021-06-25 14:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6861.3125Mb; avail=9159.5234375Mb\n",
            "2021-06-25 14:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.000925\n",
            "2021-06-25 14:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004623\n",
            "2021-06-25 14:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6861.3125Mb; avail=9159.5234375Mb\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-06-25 14:43:33 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.452 | ppl 43.77 | wps 1709.4 | wpb 203.6 | bsz 7.7 | num_updates 130 | best_loss 5.195\n",
            "2021-06-25 14:43:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 130 updates\n",
            "2021-06-25 14:43:33 | INFO | fairseq.trainer | Saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:44:31 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoint/checkpoint_last.pt\n",
            "2021-06-25 14:44:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint/checkpoint_last.pt (epoch 10 @ 130 updates, score 5.452) (writing took 58.71605838099981 seconds)\n",
            "2021-06-25 14:44:32 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2021-06-25 14:44:32 | INFO | train | epoch 010 | loss 2.764 | ppl 6.79 | wps 76.3 | ups 0.17 | wpb 443.2 | bsz 15.4 | num_updates 130 | lr 2.99992e-05 | gnorm 7.369 | train_wall 12 | gb_free 10.1 | wall 1028\n",
            "2021-06-25 14:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6868.9375Mb; avail=9147.3359375Mb\n",
            "2021-06-25 14:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000841\n",
            "2021-06-25 14:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6868.9375Mb; avail=9147.3359375Mb\n",
            "2021-06-25 14:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000086\n",
            "2021-06-25 14:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6868.9375Mb; avail=9147.3359375Mb\n",
            "2021-06-25 14:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000296\n",
            "2021-06-25 14:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002153\n",
            "2021-06-25 14:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6868.9375Mb; avail=9147.3359375Mb\n",
            "2021-06-25 14:44:32 | INFO | fairseq_cli.train | done training in 889.2 seconds\n",
            "Removed checkpoint_last.pt\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 358\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210625_142943-o4lewtd5/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210625_142943-o4lewtd5/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train_inner/ppl 7.43\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train_inner/ups 1.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train_inner/bsz 12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/gnorm 7.233\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train_inner/wps 492.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/wall 968.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train_inner/train_wall 2.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/loss 2.893\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train_inner/wpb 378.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train_inner/gb_free 10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train_inner/lr 3e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _runtime 890\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               _timestamp 1624632273\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    _step 130\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                valid/ppl 43.77\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                valid/bsz 7.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                valid/wps 1709.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               valid/loss 5.452\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                valid/wpb 203.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/ppl 6.79\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/ups 0.17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/bsz 15.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/gnorm 7.369\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/wps 76.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/wall 1028.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/train_wall 12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss 2.764\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/wpb 443.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/gb_free 10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lr 3e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          valid/best_loss 5.195\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train_inner/ppl █▆▆▅▅▃▃▄▂▂▃▃▂▃▂▂▂▂▂▂▂▁▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train_inner/ups ▇▆▇▅▁▆▆▅▁▇▅▆▁▅▇▇▁█▆▅▁▇▅▆▁▆▅▆▁▇▅▆▁▅▆▇▇▆▄▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train_inner/bsz ████▁███████▁███████▁███████▁██████████▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/gnorm █▆█▅▅▅▃▃▄▄▄▄▄▂▅▃▃▆▄▄▇▄▂▂▃▄▂▂▇▃▁▅▅▁▁▇▂▃▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train_inner/wps █▇▇█▁▇▇█▁▇█▇▁▇▇▇▁▆▆▇▁▆█▇▁▇▇█▁▇▇█▁▇█▇▆█▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/wall ▁▁▁▁▂▂▂▂▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train_inner/train_wall ▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▅▁▁▅▅▅▅▅▅▁▅▅▅▁▅▅▅▅▅█▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/loss █▇▇▇▇▆▅▆▅▅▅▆▅▅▄▄▄▄▄▄▄▃▄▃▃▃▄▃▂▂▃▃▂▂▂▂▁▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train_inner/wpb ▃▄▃▇▄▄▄▇▅▃▇▄▅█▃▄▄▂▄▇▁▃█▅▆▄▆▆▁▃▅▅▂▆▅▃▃▅█▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train_inner/gb_free ▇▆▇▆▆▃▇▇▇▆▅▅▆▇▇▇▇▆▆▆▇▇▆▇▃▆▅▄▆▆▁▇▇▅▅▇▆▄▃█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train_inner/lr ███████████▇▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _runtime ▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               _timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                valid/ppl ▄▁▂▂▂▂▂▇▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                valid/bsz ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                valid/wps █▅▅▃▃▂▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               valid/loss ▅▁▂▂▂▂▂▇▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                valid/wpb ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/ppl █▅▃▃▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/ups ▁▂█▆▆▆▆▆▆▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/bsz ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/gnorm █▄▄▃▄▂▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/wps ▁▁█▆▆▆▆▆▆▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/wall ▁▂▃▄▄▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/train_wall ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss █▆▅▄▄▃▃▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/wpb ▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/gb_free ██▅██▅█▁▅▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/lr ██▇▇▆▆▅▄▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          valid/best_loss ▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcheckpoint\u001b[0m: \u001b[34mhttps://wandb.ai/maroxtn/Yoruba%20M2M/runs/o4lewtd5\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoMhPbVXh_y2"
      },
      "source": [
        "!rm -rf data_bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSTqGqwiQoa7",
        "outputId": "cac9b5ba-02df-4e52-e332-aa8b686dfc28"
      },
      "source": [
        "!fairseq-preprocess \\\n",
        "    --source-lang yo --target-lang en \\\n",
        "    --testpref val \\\n",
        "    --thresholdsrc 0 --thresholdtgt 0 \\\n",
        "    --destdir data_bin \\\n",
        "    --srcdict data_dict.128k.txt --tgtdict data_dict.128k.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-25 14:44:49 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data_bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='yo', srcdict='data_dict.128k.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='val', tgtdict='data_dict.128k.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=1)\n",
            "2021-06-25 14:44:50 | INFO | fairseq_cli.preprocess | [yo] Dictionary: 128004 types\n",
            "2021-06-25 14:44:50 | INFO | fairseq_cli.preprocess | [yo] val.yo: 100 sents, 4576 tokens, 0.437% replaced by <unk>\n",
            "2021-06-25 14:44:50 | INFO | fairseq_cli.preprocess | [en] Dictionary: 128004 types\n",
            "2021-06-25 14:44:50 | INFO | fairseq_cli.preprocess | [en] val.en: 100 sents, 2547 tokens, 0.0393% replaced by <unk>\n",
            "2021-06-25 14:44:50 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data_bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh7O9DsYPMkR",
        "outputId": "e0368fa1-9179-41ca-ff81-be6a20d0455f"
      },
      "source": [
        "!fairseq-generate \"data_bin/\"  --batch-size 32 \\\n",
        "      --path \"checkpoint/checkpoint_best.pt\" \\\n",
        "      --fixed-dictionary model_dict.128k.txt \\\n",
        "      -s yo -t en \\\n",
        "      --remove-bpe 'sentencepiece' \\\n",
        "      --beam 5 \\\n",
        "      --task translation_multi_simple_epoch \\\n",
        "      --lang-pairs language_pairs_small_models.txt \\\n",
        "      --decoder-langtok \\\n",
        "      --encoder-langtok src \\\n",
        "      --gen-subset test > gen_out"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0% 0/4 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5Tdj1yGQ6e5"
      },
      "source": [
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP_odNz4S-_9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}